// ===============================================
// PHASE 2 DEMONSTRATION
// Model Interface Layer Demo
// ===============================================

// Phase 2 Demonstration: Model Interface Layer
// Tests the LLM API adapters and interface translation

import { 
  createModelAdapter, 
  createLLMInterface,
  PromptTemplateManager,
  ResponseParser,
  CMPMessage 
} from '../models';
import { LLMAgent } from '../core/llm-agent';
import { SemanticPose } from '../core/semantic-pose';
import { LLM_AGENT_TYPES } from '../types';
import { ConfigLoader } from '../config/config-loader';

// Mock LLM responses for testing without API keys
class MockAdapter {
  constructor(public modelType: string) {}
  
  async generateCompletion(prompt: string, options: any = {}) {
    // Simulate API delay
    await new Promise(resolve => setTimeout(resolve, 100 + Math.random() * 200));
    
    const responses: Record<string, string> = {
      'claude-3-opus': `[Claude-3-Opus Response] For the query in the prompt:

1. **Logical Analysis**: I need to break down the request systematically
2. **Key Considerations**: Multiple factors must be evaluated carefully  
3. **Reasoning Steps**: Each component requires detailed examination
4. **Conclusion**: Based on the analysis, here are the key insights

This response demonstrates structured reasoning with clear logical flow.`,

      'gpt-4-turbo': `[GPT-4-Turbo Response] Analyzing the provided prompt:

**1. Problem Decomposition**
- Breaking down the core requirements
- Identifying key constraints and objectives

**2. Solution Framework** 
- Systematic approach to address each component
- Integration of multiple perspectives

**3. Implementation Strategy**
- Practical steps for execution
- Risk mitigation considerations

This represents a comprehensive analytical approach.`,

      'claude-3-haiku': `[Claude-3-Haiku] Quick analysis:

‚Ä¢ **Core Issue**: Primary concern identification
‚Ä¢ **Solution Path**: Direct approach recommended  
‚Ä¢ **Implementation**: Streamlined execution plan

Concise but thorough response focusing on practical outcomes.`
    };
    
    return {
      id: `mock_${Date.now()}`,
      content: responses[this.modelType] || `[${this.modelType}] Mock response to: ${prompt.substring(0, 50)}...`,
      usage: {
        prompt_tokens: Math.floor(prompt.length / 4),
        completion_tokens: 150,
        total_tokens: Math.floor(prompt.length / 4) + 150
      },
      model: this.modelType,
      finish_reason: 'stop'
    };
  }
  
  getRequestCount() { return 1; }
  getCacheSize() { return 0; }
  clearCache() {}
}

export async function demonstratePhase2(): Promise<void> {
  console.log('üîå Phase 2 Demonstration: Model Interface Layer');
  console.log('================================================\n');

  try {
    // Load configuration
    const configLoader = ConfigLoader.getInstance();
    const config = configLoader.loadConfig();
    console.log('üìã Configuration loaded successfully\n');

    // 1. Test Prompt Template Manager
    console.log('1Ô∏è‚É£ Testing Prompt Template Manager...');
    await testPromptTemplates();

    // 2. Test Response Parser
    console.log('\n2Ô∏è‚É£ Testing Response Parser...');
    await testResponseParser();

    // 3. Test Model Adapters (with mocks)
    console.log('\n3Ô∏è‚É£ Testing Model Adapters...');
    await testModelAdapters();

    // 4. Test Full LLM Interface
    console.log('\n4Ô∏è‚É£ Testing LLM Interface Integration...');
    await testLLMInterface();

    console.log('\n‚úÖ Phase 2 demonstration completed successfully!');
    console.log('=====================================');
    console.log('Model Interface Layer is working correctly.');
    console.log('Ready to proceed to Phase 3: Agent Specialization');

  } catch (error) {
    console.error('\n‚ùå Phase 2 demonstration failed:', error);
    throw error;
  }
}

async function testPromptTemplates(): Promise<void> {
  const templateManager = new PromptTemplateManager();
  
  const testContext = {
    query: 'How should we implement a distributed caching system?',
    context: { 
      technology: 'Redis',
      scale: 'enterprise',
      requirements: ['high_availability', 'low_latency']
    }
  };

  // Test different agent types
  const agentTypes = [
    LLM_AGENT_TYPES.REASONING,
    LLM_AGENT_TYPES.CREATIVE,
    LLM_AGENT_TYPES.CODE,
    LLM_AGENT_TYPES.CRITIC
  ];

  for (const agentType of agentTypes) {
    const prompt = templateManager.buildPrompt(agentType, 'base', testContext);
    console.log(`   ‚úÖ ${agentType}: ${prompt.length} chars`);
    
    // Test available message types
    const messageTypes = templateManager.getAvailableMessageTypes(agentType);
    console.log(`      Message types: ${messageTypes.join(', ')}`);
  }
}

async function testResponseParser(): Promise<void> {
  const parser = new ResponseParser();
  
  const mockResponse = {
    id: 'test_123',
    content: `1. **Problem Analysis**: The distributed caching system needs to handle high throughput
2. **Architecture Design**: We should use a master-slave Redis configuration  
3. **Implementation Steps**: Start with single-node setup, then scale horizontally
4. **Risk Assessment**: Consider network partitions and data consistency issues

This approach provides a solid foundation for the caching system.`,
    usage: { total_tokens: 250, prompt_tokens: 100, completion_tokens: 150 },
    model: 'test-model',
    finish_reason: 'stop'
  };

  const parsedResponse = parser.parseToCMP(
    mockResponse,
    LLM_AGENT_TYPES.REASONING,
    new SemanticPose([1, 2, 3], 0.8, 'technical_domain')
  );

  console.log(`   ‚úÖ Parsed ${parsedResponse.reasoning.length} reasoning steps`);
  console.log(`   üìä Confidence: ${parsedResponse.confidence.toFixed(3)}`);
  console.log(`   üéØ Sample step: ${parsedResponse.reasoning[0]?.content.substring(0, 60)}...`);
}

async function testModelAdapters(): Promise<void> {
  // Test with mock adapters to avoid requiring API keys
  const adapters = [
    new MockAdapter('claude-3-opus'),
    new MockAdapter('gpt-4-turbo'),
    new MockAdapter('claude-3-haiku')
  ];

  for (const adapter of adapters) {
    const response = await adapter.generateCompletion(
      'Test prompt for model interface',
      { temperature: 0.5, maxTokens: 200 }
    );
    
    console.log(`   ‚úÖ ${adapter.modelType}: ${response.content.length} chars`);
    console.log(`   üìä Tokens: ${response.usage?.total_tokens || 0}`);
  }
}

async function testLLMInterface(): Promise<void> {
  // Create test agents with mock adapters
  const reasoningAgent = new LLMAgent(
    LLM_AGENT_TYPES.REASONING,
    'logical_analysis',
    'claude-3-opus'
  );

  const creativeAgent = new LLMAgent(
    LLM_AGENT_TYPES.CREATIVE,
    'creative_synthesis', 
    'gpt-4-turbo'
  );

  // Create interfaces with mock adapters
  const reasoningInterface = new (class {
    constructor(private agent: LLMAgent, private adapter: MockAdapter) {}
    
    async processCMPMessage(message: CMPMessage, context: any) {
      const response = await this.adapter.generateCompletion('test', {});
      const parser = new ResponseParser();
      return parser.parseToCMP(response, this.agent.type);
    }
    
    getModelInfo() {
      return { 
        modelType: this.adapter.modelType,
        requestCount: this.adapter.getRequestCount(),
        cacheSize: this.adapter.getCacheSize()
      };
    }
  })(reasoningAgent, new MockAdapter('claude-3-opus'));

  const creativeInterface = new (class {
    constructor(private agent: LLMAgent, private adapter: MockAdapter) {}
    
    async processCMPMessage(message: CMPMessage, context: any) {
      const response = await this.adapter.generateCompletion('test', {});
      const parser = new ResponseParser();
      return parser.parseToCMP(response, this.agent.type);
    }
    
    getModelInfo() {
      return { 
        modelType: this.adapter.modelType,
        requestCount: this.adapter.getRequestCount(),
        cacheSize: this.adapter.getCacheSize()
      };
    }
  })(creativeAgent, new MockAdapter('gpt-4-turbo'));

  // Test CMP message processing
  const testMessage: CMPMessage = {
    type: 'QUERY_ANALYSIS',
    reasoning: [{
      type: 'query',
      concept: 'distributed_systems',
      content: 'How should we design a fault-tolerant microservices architecture?',
      confidence: 0.8
    }],
    semantic_pose: new SemanticPose([45, 67, 23], 0.8, 'technical_domain'),
    confidence: 0.8
  };

  const context = {
    query: 'How should we design a fault-tolerant microservices architecture?',
    domain: 'software_engineering',
    constraints: ['high_availability', 'scalability']
  };

  // Process with different agents
  console.log('   üî¨ Processing with REASONING agent...');
  const reasoningResponse = await reasoningInterface.processCMPMessage(testMessage, context);
  
  console.log('   üé® Processing with CREATIVE agent...');
  const creativeResponse = await creativeInterface.processCMPMessage(testMessage, context);

  // Display results
  console.log('\n   üìä INTERFACE RESULTS:');
  console.log('   ====================');
  
  console.log(`   üî¨ REASONING: ${reasoningResponse.reasoning.length} steps, confidence ${reasoningResponse.confidence.toFixed(3)}`);
  console.log(`   üé® CREATIVE: ${creativeResponse.reasoning.length} steps, confidence ${creativeResponse.confidence.toFixed(3)}`);
  
  console.log('\n   üîÑ MODEL INFO:');
  const reasoningInfo = reasoningInterface.getModelInfo();
  const creativeInfo = creativeInterface.getModelInfo();
  console.log(`   Reasoning: ${reasoningInfo.modelType} (${reasoningInfo.requestCount} requests)`);
  console.log(`   Creative: ${creativeInfo.modelType} (${creativeInfo.requestCount} requests)`);
}

// Run demonstration if called directly
if (require.main === module) {
  demonstratePhase2().catch(console.error);
}
